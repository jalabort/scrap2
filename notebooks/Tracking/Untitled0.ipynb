{
 "metadata": {
  "name": "",
  "signature": "sha256:109a623848ea6949f79b0e9d6a04323eaa942365728ef9d888bb2c4ed9d5309c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "# See also: http://sundararajana.blogspot.com/2007/05/motion-detection-using-opencv.html\n",
      "\n",
      "import cv\n",
      "import time\n",
      "\n",
      "from scipy import *\n",
      "from scipy.cluster import vq\n",
      "import numpy\n",
      "import sys, os, random, hashlib\n",
      "\n",
      "from math import *\n",
      "\n",
      "\"\"\"\n",
      "Python Motion Tracker\n",
      "\n",
      "Reads an incoming video stream and tracks motion in real time.\n",
      "Detected motion events are logged to a text file.  Also has face detection.\n",
      "\"\"\"\n",
      "\n",
      "#\n",
      "# BBoxes must be in the format:\n",
      "# ( (topleft_x), (topleft_y) ), ( (bottomright_x), (bottomright_y) ) )\n",
      "top = 0\n",
      "bottom = 1\n",
      "left = 0\n",
      "right = 1\n",
      "\n",
      "def merge_collided_bboxes( bbox_list ):\n",
      "\t# For every bbox...\n",
      "\tfor this_bbox in bbox_list:\n",
      "\t\t\n",
      "\t\t# Collision detect every other bbox:\n",
      "\t\tfor other_bbox in bbox_list:\n",
      "\t\t\tif this_bbox is other_bbox: continue  # Skip self\n",
      "\t\t\t\n",
      "\t\t\t# Assume a collision to start out with:\n",
      "\t\t\thas_collision = True\n",
      "\t\t\t\n",
      "\t\t\t# These coords are in screen coords, so > means \n",
      "\t\t\t# \"lower than\" and \"further right than\".  And < \n",
      "\t\t\t# means \"higher than\" and \"further left than\".\n",
      "\t\t\t\n",
      "\t\t\t# We also inflate the box size by 10% to deal with\n",
      "\t\t\t# fuzziness in the data.  (Without this, there are many times a bbox\n",
      "\t\t\t# is short of overlap by just one or two pixels.)\n",
      "\t\t\tif (this_bbox[bottom][0]*1.1 < other_bbox[top][0]*0.9): has_collision = False\n",
      "\t\t\tif (this_bbox[top][0]*.9 > other_bbox[bottom][0]*1.1): has_collision = False\n",
      "\t\t\t\n",
      "\t\t\tif (this_bbox[right][1]*1.1 < other_bbox[left][1]*0.9): has_collision = False\n",
      "\t\t\tif (this_bbox[left][1]*0.9 > other_bbox[right][1]*1.1): has_collision = False\n",
      "\t\t\t\n",
      "\t\t\tif has_collision:\n",
      "\t\t\t\t# merge these two bboxes into one, then start over:\n",
      "\t\t\t\ttop_left_x = min( this_bbox[left][0], other_bbox[left][0] )\n",
      "\t\t\t\ttop_left_y = min( this_bbox[left][1], other_bbox[left][1] )\n",
      "\t\t\t\tbottom_right_x = max( this_bbox[right][0], other_bbox[right][0] )\n",
      "\t\t\t\tbottom_right_y = max( this_bbox[right][1], other_bbox[right][1] )\n",
      "\t\t\t\t\n",
      "\t\t\t\tnew_bbox = ( (top_left_x, top_left_y), (bottom_right_x, bottom_right_y) )\n",
      "\t\t\t\t\n",
      "\t\t\t\tbbox_list.remove( this_bbox )\n",
      "\t\t\t\tbbox_list.remove( other_bbox )\n",
      "\t\t\t\tbbox_list.append( new_bbox )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Start over with the new list:\n",
      "\t\t\t\treturn merge_collided_bboxes( bbox_list )\n",
      "\t\n",
      "\t# When there are no collions between boxes, return that list:\n",
      "\treturn bbox_list\n",
      "\n",
      "\n",
      "def detect_faces( image, haar_cascade, mem_storage ):\n",
      "\n",
      "\tfaces = []\n",
      "\timage_size = cv.GetSize( image )\n",
      "\n",
      "\t#faces = cv.HaarDetectObjects(grayscale, haar_cascade, storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, (20, 20) )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, storage )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( 16, 16 ) )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( 4,4 ) )\n",
      "\tfaces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( image_size[0]/10, image_size[1]/10) )\n",
      "\t\n",
      "\tfor face in faces:\n",
      "\t\tbox = face[0]\n",
      "\t\tcv.Rectangle(image, ( box[0], box[1] ),\n",
      "\t\t\t( box[0] + box[2], box[1] + box[3]), cv.RGB(255, 0, 0), 1, 8, 0)\n",
      "\n",
      "\n",
      "class Target:\n",
      "\tdef __init__(self):\n",
      "\t\t\n",
      "\t\tif len( sys.argv ) > 1:\n",
      "\t\t\tself.writer = None\n",
      "\t\t\tself.capture = cv.CaptureFromFile( sys.argv[1] )\n",
      "\t\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\t\tframe_size = cv.GetSize(frame)\n",
      "\t\telse:\n",
      "\t\t\tfps=15\n",
      "\t\t\tis_color = True\n",
      "\n",
      "\t\t\tself.capture = cv.CaptureFromCAM(0)\n",
      "\t\t\t#cv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_WIDTH, 640 );\n",
      "\t\t\t#cv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_HEIGHT, 480 );\n",
      "\t\t\tcv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_WIDTH, 320 );\n",
      "\t\t\tcv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_HEIGHT, 240 );\n",
      "\t\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\t\tframe_size = cv.GetSize(frame)\n",
      "\t\t\t\n",
      "\t\t\tself.writer = None\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"/dev/shm/test1.mp4\", cv.CV_FOURCC('D', 'I', 'V', 'X'), fps, frame_size, is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test2.mpg\", cv.CV_FOURCC('P', 'I', 'M', '1'), fps, frame_size, is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test3.mp4\", cv.CV_FOURCC('D', 'I', 'V', 'X'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test4.mpg\", cv.CV_FOURCC('P', 'I', 'M', '1'), fps, (320, 240), is_color )\n",
      "\t\t\t\n",
      "\t\t\t# These both gave no error message, but saved no file:\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test5.h263i\", cv.CV_FOURCC('I', '2', '6', '3'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test6.fli\",   cv.CV_FOURCC('F', 'L', 'V', '1'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t# Can't play this one:\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test7.mp4\",   cv.CV_FOURCC('D', 'I', 'V', '3'), fps, cv.GetSize(frame), is_color )\n",
      "\n",
      "\t\t# 320x240 15fpx in DIVX is about 4 gigs per day.\n",
      "\n",
      "\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\tcv.NamedWindow(\"Target\", 1)\n",
      "\t\t#cv.NamedWindow(\"Target2\", 1)\n",
      "\t\t\n",
      "\n",
      "\tdef run(self):\n",
      "\t\t# Initialize\n",
      "\t\t#log_file_name = \"tracker_output.log\"\n",
      "\t\t#log_file = file( log_file_name, 'a' )\n",
      "\t\t\n",
      "\t\tframe = cv.QueryFrame( self.capture )\n",
      "\t\tframe_size = cv.GetSize( frame )\n",
      "\t\t\n",
      "\t\t# Capture the first frame from webcam for image properties\n",
      "\t\tdisplay_image = cv.QueryFrame( self.capture )\n",
      "\t\t\n",
      "\t\t# Greyscale image, thresholded to create the motion mask:\n",
      "\t\tgrey_image = cv.CreateImage( cv.GetSize(frame), cv.IPL_DEPTH_8U, 1 )\n",
      "\t\t\n",
      "\t\t# The RunningAvg() function requires a 32-bit or 64-bit image...\n",
      "\t\trunning_average_image = cv.CreateImage( cv.GetSize(frame), cv.IPL_DEPTH_32F, 3 )\n",
      "\t\t# ...but the AbsDiff() function requires matching image depths:\n",
      "\t\trunning_average_in_display_color_depth = cv.CloneImage( display_image )\n",
      "\t\t\n",
      "\t\t# RAM used by FindContours():\n",
      "\t\tmem_storage = cv.CreateMemStorage(0)\n",
      "\t\t\n",
      "\t\t# The difference between the running average and the current frame:\n",
      "\t\tdifference = cv.CloneImage( display_image )\n",
      "\t\t\n",
      "\t\ttarget_count = 1\n",
      "\t\tlast_target_count = 1\n",
      "\t\tlast_target_change_t = 0.0\n",
      "\t\tk_or_guess = 1\n",
      "\t\tcodebook=[]\n",
      "\t\tframe_count=0\n",
      "\t\tlast_frame_entity_list = []\n",
      "\t\t\n",
      "\t\tt0 = time.time()\n",
      "\t\t\n",
      "\t\t# For toggling display:\n",
      "\t\timage_list = [ \"camera\", \"difference\", \"threshold\", \"display\", \"faces\" ]\n",
      "\t\timage_index = 0   # Index into image_list\n",
      "\t\n",
      "\t\n",
      "\t\t# Prep for text drawing:\n",
      "\t\ttext_font = cv.InitFont(cv.CV_FONT_HERSHEY_COMPLEX, .5, .5, 0.0, 1, cv.CV_AA )\n",
      "\t\ttext_coord = ( 5, 15 )\n",
      "\t\ttext_color = cv.CV_RGB(255,255,255)\n",
      "\n",
      "\t\t###############################\n",
      "\t\t### Face detection stuff\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_default.xml' )\n",
      "\t\thaar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt2.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_mcs_mouth.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_eye.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt_tree.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_upperbody.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_profileface.xml' )\n",
      "\t\t\n",
      "\t\t# Set this to the max number of targets to look for (passed to k-means):\n",
      "\t\tmax_targets = 3\n",
      "\t\t\n",
      "\t\twhile True:\n",
      "\t\t\t\n",
      "\t\t\t# Capture frame from webcam\n",
      "\t\t\tcamera_image = cv.QueryFrame( self.capture )\n",
      "\t\t\t\n",
      "\t\t\tframe_count += 1\n",
      "\t\t\tframe_t0 = time.time()\n",
      "\t\t\t\n",
      "\t\t\t# Create an image with interactive feedback:\n",
      "\t\t\tdisplay_image = cv.CloneImage( camera_image )\n",
      "\t\t\t\n",
      "\t\t\t# Create a working \"color image\" to modify / blur\n",
      "\t\t\tcolor_image = cv.CloneImage( display_image )\n",
      "\n",
      "\t\t\t# Smooth to get rid of false positives\n",
      "\t\t\tcv.Smooth( color_image, color_image, cv.CV_GAUSSIAN, 19, 0 )\n",
      "\t\t\t\n",
      "\t\t\t# Use the Running Average as the static background\t\t\t\n",
      "\t\t\t# a = 0.020 leaves artifacts lingering way too long.\n",
      "\t\t\t# a = 0.320 works well at 320x240, 15fps.  (1/a is roughly num frames.)\n",
      "\t\t\tcv.RunningAvg( color_image, running_average_image, 0.320, None )\n",
      "\t\t\t\n",
      "\t\t\t# Convert the scale of the moving average.\n",
      "\t\t\tcv.ConvertScale( running_average_image, running_average_in_display_color_depth, 1.0, 0.0 )\n",
      "\t\t\t\n",
      "\t\t\t# Subtract the current frame from the moving average.\n",
      "\t\t\tcv.AbsDiff( color_image, running_average_in_display_color_depth, difference )\n",
      "\t\t\t\n",
      "\t\t\t# Convert the image to greyscale.\n",
      "\t\t\tcv.CvtColor( difference, grey_image, cv.CV_RGB2GRAY )\n",
      "\n",
      "\t\t\t# Threshold the image to a black and white motion mask:\n",
      "\t\t\tcv.Threshold( grey_image, grey_image, 2, 255, cv.CV_THRESH_BINARY )\n",
      "\t\t\t# Smooth and threshold again to eliminate \"sparkles\"\n",
      "\t\t\tcv.Smooth( grey_image, grey_image, cv.CV_GAUSSIAN, 19, 0 )\n",
      "\t\t\tcv.Threshold( grey_image, grey_image, 240, 255, cv.CV_THRESH_BINARY )\n",
      "\t\t\t\n",
      "\t\t\tgrey_image_as_array = numpy.asarray( cv.GetMat( grey_image ) )\n",
      "\t\t\tnon_black_coords_array = numpy.where( grey_image_as_array > 3 )\n",
      "\t\t\t# Convert from numpy.where()'s two separate lists to one list of (x, y) tuples:\n",
      "\t\t\tnon_black_coords_array = zip( non_black_coords_array[1], non_black_coords_array[0] )\n",
      "\t\t\t\n",
      "\t\t\tpoints = []   # Was using this to hold either pixel coords or polygon coords.\n",
      "\t\t\tbounding_box_list = []\n",
      "\n",
      "\t\t\t# Now calculate movements using the white pixels as \"motion\" data\n",
      "\t\t\tcontour = cv.FindContours( grey_image, mem_storage, cv.CV_RETR_CCOMP, cv.CV_CHAIN_APPROX_SIMPLE )\n",
      "\t\t\t\n",
      "\t\t\twhile contour:\n",
      "\t\t\t\t\n",
      "\t\t\t\tbounding_rect = cv.BoundingRect( list(contour) )\n",
      "\t\t\t\tpoint1 = ( bounding_rect[0], bounding_rect[1] )\n",
      "\t\t\t\tpoint2 = ( bounding_rect[0] + bounding_rect[2], bounding_rect[1] + bounding_rect[3] )\n",
      "\t\t\t\t\n",
      "\t\t\t\tbounding_box_list.append( ( point1, point2 ) )\n",
      "\t\t\t\tpolygon_points = cv.ApproxPoly( list(contour), mem_storage, cv.CV_POLY_APPROX_DP )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# To track polygon points only (instead of every pixel):\n",
      "\t\t\t\t#points += list(polygon_points)\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Draw the contours:\n",
      "\t\t\t\t###cv.DrawContours(color_image, contour, cv.CV_RGB(255,0,0), cv.CV_RGB(0,255,0), levels, 3, 0, (0,0) )\n",
      "\t\t\t\tcv.FillPoly( grey_image, [ list(polygon_points), ], cv.CV_RGB(255,255,255), 0, 0 )\n",
      "\t\t\t\tcv.PolyLine( display_image, [ polygon_points, ], 0, cv.CV_RGB(255,255,255), 1, 0, 0 )\n",
      "\t\t\t\t#cv.Rectangle( display_image, point1, point2, cv.CV_RGB(120,120,120), 1)\n",
      "\n",
      "\t\t\t\tcontour = contour.h_next()\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t# Find the average size of the bbox (targets), then\n",
      "\t\t\t# remove any tiny bboxes (which are prolly just noise).\n",
      "\t\t\t# \"Tiny\" is defined as any box with 1/10th the area of the average box.\n",
      "\t\t\t# This reduces false positives on tiny \"sparkles\" noise.\n",
      "\t\t\tbox_areas = []\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tbox_width = box[right][0] - box[left][0]\n",
      "\t\t\t\tbox_height = box[bottom][0] - box[top][0]\n",
      "\t\t\t\tbox_areas.append( box_width * box_height )\n",
      "\t\t\t\t\n",
      "\t\t\t\t#cv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(255,0,0), 1)\n",
      "\t\t\t\n",
      "\t\t\taverage_box_area = 0.0\n",
      "\t\t\tif len(box_areas): average_box_area = float( sum(box_areas) ) / len(box_areas)\n",
      "\t\t\t\n",
      "\t\t\ttrimmed_box_list = []\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tbox_width = box[right][0] - box[left][0]\n",
      "\t\t\t\tbox_height = box[bottom][0] - box[top][0]\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Only keep the box if it's not a tiny noise box:\n",
      "\t\t\t\tif (box_width * box_height) > average_box_area*0.1: trimmed_box_list.append( box )\n",
      "\t\t\t\n",
      "\t\t\t# Draw the trimmed box list:\n",
      "\t\t\t#for box in trimmed_box_list:\n",
      "\t\t\t#\tcv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(0,255,0), 2 )\n",
      "\t\t\t\t\n",
      "\t\t\tbounding_box_list = merge_collided_bboxes( trimmed_box_list )\n",
      "\n",
      "\t\t\t# Draw the merged box list:\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tcv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(0,255,0), 1 )\n",
      "\t\t\t\n",
      "\t\t\t# Here are our estimate points to track, based on merged & trimmed boxes:\n",
      "\t\t\testimated_target_count = len( bounding_box_list )\n",
      "\t\t\t\n",
      "\t\t\t# Don't allow target \"jumps\" from few to many or many to few.\n",
      "\t\t\t# Only change the number of targets up to one target per n seconds.\n",
      "\t\t\t# This fixes the \"exploding number of targets\" when something stops moving\n",
      "\t\t\t# and the motion erodes to disparate little puddles all over the place.\n",
      "\t\t\t\n",
      "\t\t\tif frame_t0 - last_target_change_t < .350:  # 1 change per 0.35 secs\n",
      "\t\t\t\testimated_target_count = last_target_count\n",
      "\t\t\telse:\n",
      "\t\t\t\tif last_target_count - estimated_target_count > 1: estimated_target_count = last_target_count - 1\n",
      "\t\t\t\tif estimated_target_count - last_target_count > 1: estimated_target_count = last_target_count + 1\n",
      "\t\t\t\tlast_target_change_t = frame_t0\n",
      "\t\t\t\n",
      "\t\t\t# Clip to the user-supplied maximum:\n",
      "\t\t\testimated_target_count = min( estimated_target_count, max_targets )\n",
      "\t\t\t\n",
      "\t\t\t# The estimated_target_count at this point is the maximum number of targets\n",
      "\t\t\t# we want to look for.  If kmeans decides that one of our candidate\n",
      "\t\t\t# bboxes is not actually a target, we remove it from the target list below.\n",
      "\t\t\t\n",
      "\t\t\t# Using the numpy values directly (treating all pixels as points):\t\n",
      "\t\t\tpoints = non_black_coords_array\n",
      "\t\t\tcenter_points = []\n",
      "\t\t\t\n",
      "\t\t\tif len(points):\n",
      "\t\t\t\t\n",
      "\t\t\t\t# If we have all the \"target_count\" targets from last frame,\n",
      "\t\t\t\t# use the previously known targets (for greater accuracy).\n",
      "\t\t\t\tk_or_guess = max( estimated_target_count, 1 )  # Need at least one target to look for.\n",
      "\t\t\t\tif len(codebook) == estimated_target_count: \n",
      "\t\t\t\t\tk_or_guess = codebook\n",
      "\t\t\t\t\n",
      "\t\t\t\t#points = vq.whiten(array( points ))  # Don't do this!  Ruins everything.\n",
      "\t\t\t\tcodebook, distortion = vq.kmeans( array( points ), k_or_guess )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Convert to tuples (and draw it to screen)\n",
      "\t\t\t\tfor center_point in codebook:\n",
      "\t\t\t\t\tcenter_point = ( int(center_point[0]), int(center_point[1]) )\n",
      "\t\t\t\t\tcenter_points.append( center_point )\n",
      "\t\t\t\t\t#cv.Circle(display_image, center_point, 10, cv.CV_RGB(255, 0, 0), 2)\n",
      "\t\t\t\t\t#cv.Circle(display_image, center_point, 5, cv.CV_RGB(255, 0, 0), 3)\n",
      "\t\t\t\n",
      "\t\t\t# Now we have targets that are NOT computed from bboxes -- just\n",
      "\t\t\t# movement weights (according to kmeans).  If any two targets are\n",
      "\t\t\t# within the same \"bbox count\", average them into a single target.  \n",
      "\t\t\t#\n",
      "\t\t\t# (Any kmeans targets not within a bbox are also kept.)\n",
      "\t\t\ttrimmed_center_points = []\n",
      "\t\t\tremoved_center_points = []\n",
      "\t\t\t\t\t\t\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\t# Find the centers within this box:\n",
      "\t\t\t\tcenter_points_in_box = []\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor center_point in center_points:\n",
      "\t\t\t\t\tif\tcenter_point[0] < box[right][0] and center_point[0] > box[left][0] and \\\n",
      "\t\t\t\t\t\tcenter_point[1] < box[bottom][1] and center_point[1] > box[top][1] :\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t# This point is within the box.\n",
      "\t\t\t\t\t\tcenter_points_in_box.append( center_point )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Now see if there are more than one.  If so, merge them.\n",
      "\t\t\t\tif len( center_points_in_box ) > 1:\n",
      "\t\t\t\t\t# Merge them:\n",
      "\t\t\t\t\tx_list = y_list = []\n",
      "\t\t\t\t\tfor point in center_points_in_box:\n",
      "\t\t\t\t\t\tx_list.append(point[0])\n",
      "\t\t\t\t\t\ty_list.append(point[1])\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\taverage_x = int( float(sum( x_list )) / len( x_list ) )\n",
      "\t\t\t\t\taverage_y = int( float(sum( y_list )) / len( y_list ) )\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\ttrimmed_center_points.append( (average_x, average_y) )\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Record that they were removed:\n",
      "\t\t\t\t\tremoved_center_points += center_points_in_box\n",
      "\t\t\t\t\t\n",
      "\t\t\t\tif len( center_points_in_box ) == 1:\n",
      "\t\t\t\t\ttrimmed_center_points.append( center_points_in_box[0] ) # Just use it.\n",
      "\t\t\t\n",
      "\t\t\t# If there are any center_points not within a bbox, just use them.\n",
      "\t\t\t# (It's probably a cluster comprised of a bunch of small bboxes.)\n",
      "\t\t\tfor center_point in center_points:\n",
      "\t\t\t\tif (not center_point in trimmed_center_points) and (not center_point in removed_center_points):\n",
      "\t\t\t\t\ttrimmed_center_points.append( center_point )\n",
      "\t\t\t\n",
      "\t\t\t# Draw what we found:\n",
      "\t\t\t#for center_point in trimmed_center_points:\n",
      "\t\t\t#\tcenter_point = ( int(center_point[0]), int(center_point[1]) )\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 20, cv.CV_RGB(255, 255,255), 1)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 15, cv.CV_RGB(100, 255, 255), 1)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 10, cv.CV_RGB(255, 255, 255), 2)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 5, cv.CV_RGB(100, 255, 255), 3)\n",
      "\t\t\t\n",
      "\t\t\t# Determine if there are any new (or lost) targets:\n",
      "\t\t\tactual_target_count = len( trimmed_center_points )\n",
      "\t\t\tlast_target_count = actual_target_count\n",
      "\t\t\t\n",
      "\t\t\t# Now build the list of physical entities (objects)\n",
      "\t\t\tthis_frame_entity_list = []\n",
      "\t\t\t\n",
      "\t\t\t# An entity is list: [ name, color, last_time_seen, last_known_coords ]\n",
      "\t\t\t\n",
      "\t\t\tfor target in trimmed_center_points:\n",
      "\t\t\t\n",
      "\t\t\t\t# Is this a target near a prior entity (same physical entity)?\n",
      "\t\t\t\tentity_found = False\n",
      "\t\t\t\tentity_distance_dict = {}\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor entity in last_frame_entity_list:\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tentity_coords= entity[3]\n",
      "\t\t\t\t\tdelta_x = entity_coords[0] - target[0]\n",
      "\t\t\t\t\tdelta_y = entity_coords[1] - target[1]\n",
      "\t\t\t\n",
      "\t\t\t\t\tdistance = sqrt( pow(delta_x,2) + pow( delta_y,2) )\n",
      "\t\t\t\t\tentity_distance_dict[ distance ] = entity\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Did we find any non-claimed entities (nearest to furthest):\n",
      "\t\t\t\tdistance_list = entity_distance_dict.keys()\n",
      "\t\t\t\tdistance_list.sort()\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor distance in distance_list:\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Yes; see if we can claim the nearest one:\n",
      "\t\t\t\t\tnearest_possible_entity = entity_distance_dict[ distance ]\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Don't consider entities that are already claimed:\n",
      "\t\t\t\t\tif nearest_possible_entity in this_frame_entity_list:\n",
      "\t\t\t\t\t\t#print \"Target %s: Skipping the one iwth distance: %d at %s, C:%s\" % (target, distance, nearest_possible_entity[3], nearest_possible_entity[1] )\n",
      "\t\t\t\t\t\tcontinue\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t#print \"Target %s: USING the one iwth distance: %d at %s, C:%s\" % (target, distance, nearest_possible_entity[3] , nearest_possible_entity[1])\n",
      "\t\t\t\t\t# Found the nearest entity to claim:\n",
      "\t\t\t\t\tentity_found = True\n",
      "\t\t\t\t\tnearest_possible_entity[2] = frame_t0  # Update last_time_seen\n",
      "\t\t\t\t\tnearest_possible_entity[3] = target  # Update the new location\n",
      "\t\t\t\t\tthis_frame_entity_list.append( nearest_possible_entity )\n",
      "\t\t\t\t\t#log_file.write( \"%.3f MOVED %s %d %d\\n\" % ( frame_t0, nearest_possible_entity[0], nearest_possible_entity[3][0], nearest_possible_entity[3][1]  ) )\n",
      "\t\t\t\t\tbreak\n",
      "\t\t\t\t\n",
      "\t\t\t\tif entity_found == False:\n",
      "\t\t\t\t\t# It's a new entity.\n",
      "\t\t\t\t\tcolor = ( random.randint(0,255), random.randint(0,255), random.randint(0,255) )\n",
      "\t\t\t\t\tname = hashlib.md5( str(frame_t0) + str(color) ).hexdigest()[:6]\n",
      "\t\t\t\t\tlast_time_seen = frame_t0\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tnew_entity = [ name, color, last_time_seen, target ]\n",
      "\t\t\t\t\tthis_frame_entity_list.append( new_entity )\n",
      "\t\t\t\t\t#log_file.write( \"%.3f FOUND %s %d %d\\n\" % ( frame_t0, new_entity[0], new_entity[3][0], new_entity[3][1]  ) )\n",
      "\t\t\t\n",
      "\t\t\t# Now \"delete\" any not-found entities which have expired:\n",
      "\t\t\tentity_ttl = 1.0  # 1 sec.\n",
      "\t\t\t\n",
      "\t\t\tfor entity in last_frame_entity_list:\n",
      "\t\t\t\tlast_time_seen = entity[2]\n",
      "\t\t\t\tif frame_t0 - last_time_seen > entity_ttl:\n",
      "\t\t\t\t\t# It's gone.\n",
      "\t\t\t\t\t#log_file.write( \"%.3f STOPD %s %d %d\\n\" % ( frame_t0, entity[0], entity[3][0], entity[3][1]  ) )\n",
      "\t\t\t\t\tpass\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\t# Save it for next time... not expired yet:\n",
      "\t\t\t\t\tthis_frame_entity_list.append( entity )\n",
      "\t\t\t\n",
      "\t\t\t# For next frame:\n",
      "\t\t\tlast_frame_entity_list = this_frame_entity_list\n",
      "\t\t\t\n",
      "\t\t\t# Draw the found entities to screen:\n",
      "\t\t\tfor entity in this_frame_entity_list:\n",
      "\t\t\t\tcenter_point = entity[3]\n",
      "\t\t\t\tc = entity[1]  # RGB color tuple\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 20, cv.CV_RGB(c[0], c[1], c[2]), 1)\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 15, cv.CV_RGB(c[0], c[1], c[2]), 1)\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 10, cv.CV_RGB(c[0], c[1], c[2]), 2)\n",
      "\t\t\t\tcv.Circle(display_image, center_point,  5, cv.CV_RGB(c[0], c[1], c[2]), 3)\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t#print \"min_size is: \" + str(min_size)\n",
      "\t\t\t# Listen for ESC or ENTER key\n",
      "\t\t\tc = cv.WaitKey(7) % 0x100\n",
      "\t\t\tif c == 27 or c == 10:\n",
      "\t\t\t\tbreak\n",
      "\t\t\t\n",
      "\t\t\t# Toggle which image to show\n",
      "\t\t\tif chr(c) == 'd':\n",
      "\t\t\t\timage_index = ( image_index + 1 ) % len( image_list )\n",
      "\t\t\t\n",
      "\t\t\timage_name = image_list[ image_index ]\n",
      "\t\t\t\n",
      "\t\t\t# Display frame to user\n",
      "\t\t\tif image_name == \"camera\":\n",
      "\t\t\t\timage = camera_image\n",
      "\t\t\t\tcv.PutText( image, \"Camera (Normal)\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"difference\":\n",
      "\t\t\t\timage = difference\n",
      "\t\t\t\tcv.PutText( image, \"Difference Image\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"display\":\n",
      "\t\t\t\timage = display_image\n",
      "\t\t\t\tcv.PutText( image, \"Targets (w/AABBs and contours)\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"threshold\":\n",
      "\t\t\t\t# Convert the image to color.\n",
      "\t\t\t\tcv.CvtColor( grey_image, display_image, cv.CV_GRAY2RGB )\n",
      "\t\t\t\timage = display_image  # Re-use display image here\n",
      "\t\t\t\tcv.PutText( image, \"Motion Mask\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"faces\":\n",
      "\t\t\t\t# Do face detection\n",
      "\t\t\t\tdetect_faces( camera_image, haar_cascade, mem_storage )\t\t\t\t\n",
      "\t\t\t\timage = camera_image  # Re-use camera image here\n",
      "\t\t\t\tcv.PutText( image, \"Face Detection\", text_coord, text_font, text_color )\n",
      "\t\t\t\n",
      "\t\t\tcv.ShowImage( \"Target\", image )\n",
      "\t\t\t\n",
      "\t\t\tif self.writer: \n",
      "\t\t\t\tcv.WriteFrame( self.writer, image );\n",
      "\t\t\t\n",
      "\t\t\t#log_file.flush()\n",
      "\t\t\t\n",
      "\t\t\t# If only using a camera, then there is no time.sleep() needed, \n",
      "\t\t\t# because the camera clips us to 15 fps.  But if reading from a file,\n",
      "\t\t\t# we need this to keep the time-based target clipping correct:\n",
      "\t\t\tframe_t1 = time.time()\n",
      "\t\t\t\n",
      "\n",
      "\t\t\t# If reading from a file, put in a forced delay:\n",
      "\t\t\tif not self.writer:\n",
      "\t\t\t\tdelta_t = frame_t1 - frame_t0\n",
      "\t\t\t\tif delta_t < ( 1.0 / 15.0 ): time.sleep( ( 1.0 / 15.0 ) - delta_t )\n",
      "\t\t\t\n",
      "\t\tt1 = time.time()\n",
      "\t\ttime_delta = t1 - t0\n",
      "\t\tprocessed_fps = float( frame_count ) / time_delta\n",
      "\t\tprint \"Got %d frames. %.1f s. %f fps.\" % ( frame_count, time_delta, processed_fps )\n",
      "\t\t\n",
      "if __name__==\"__main__\":\n",
      "\tt = Target()\n",
      "#\timport cProfile\n",
      "#\tcProfile.run( 't.run()' )\n",
      "\tt.run()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named cv2",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-48425add5976>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# See also: http://sundararajana.blogspot.com/2007/05/motion-detection-using-opencv.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named cv2"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}