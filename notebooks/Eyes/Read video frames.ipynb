{
 "metadata": {
  "name": "",
  "signature": "sha256:feea9dffa51add6012fa1074fcc2b831e6cef6738bd53ceb5c8d25a0fabb26d2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "\n",
      "vidcap = cv2.VideoCapture('/Users/joan/PhD/DataBases/Eye_vid.avi')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import abc\n",
      "import cv2\n",
      "import numpy as np\n",
      "import time\n",
      "\n",
      "\n",
      "class CaptureManager(object):\n",
      "\n",
      "    def __init__(self, capture, preview_window_manager=None,\n",
      "                 should_mirror_preview=False):\n",
      "        self.preview_window_manager = preview_window_manager\n",
      "        self.should_mirror_preview = should_mirror_preview\n",
      "        self._capture = capture\n",
      "        self._channel = 0\n",
      "        self._enteredFrame = False\n",
      "        self._frame = None\n",
      "        self._image_filename = None\n",
      "        self._video_filename = None\n",
      "        self._video_encoding = None\n",
      "        self._video_writer = None\n",
      "        self._start_time = None\n",
      "        self._frames_elapsed = long(0)\n",
      "        self._fps_estimate = None\n",
      "\n",
      "    @property\n",
      "    def channel(self):\n",
      "        return self._channel\n",
      "\n",
      "    @channel.setter\n",
      "    def channel(self, value):\n",
      "        if self._channel != value:\n",
      "            self._channel = value\n",
      "            self._frame = None\n",
      "\n",
      "    @property\n",
      "    def frame(self):\n",
      "        if self._enteredFrame and self._frame is None:\n",
      "            _, self._frame = self._capture.retrieve(channel=self.channel)\n",
      "        return self._frame\n",
      "\n",
      "    @property\n",
      "    def is_writing_image(self):\n",
      "        return self._image_filename is not None\n",
      "\n",
      "    @property\n",
      "    def is_writing_video(self):\n",
      "        return self._video_filename is not None\n",
      "\n",
      "    def enter_frame(self):\n",
      "        \"\"\"Capture the next frame, if any.\"\"\"\n",
      "        # But first, check that any previous frame was exited.\n",
      "        assert not self._enteredFrame, 'previous enterFrame() had ' \\\n",
      "                                       'no matching exitFrame()'\n",
      "        if self._capture is not None:\n",
      "            self._enteredFrame = self._capture.grab()\n",
      "\n",
      "    def exit_frame(self):\n",
      "        \"\"\"Draw to the window. Write to files. Release the frame.\"\"\"\n",
      "        # Check whether any grabbed frame is retrievable.\n",
      "        # The getter may retrieve and cache the frame.\n",
      "        if self.frame is None:\n",
      "            self._enteredFrame = False\n",
      "            return\n",
      "        # Update the FPS estimate and related variables.\n",
      "        if self._frames_elapsed == 0:\n",
      "            self._start_time = time.time()\n",
      "        else:\n",
      "            time_elapsed = time.time() - self._start_time\n",
      "            self._fps_estimate = self._frames_elapsed / time_elapsed\n",
      "        self._frames_elapsed += 1\n",
      "        # Draw to the window, if any.\n",
      "        if self.preview_window_manager is not None:\n",
      "            if self.should_mirror_preview:\n",
      "                mirrored_frame = np.fliplr(self._frame).copy()\n",
      "                self.preview_window_manager.show(mirrored_frame)\n",
      "            else:\n",
      "                self.preview_window_manager.show(self._frame)\n",
      "        # Write to the image file, if any.\n",
      "        if self.is_writing_image:\n",
      "            cv2.imwrite(self._image_filename, self._frame)\n",
      "            self._image_filename = None\n",
      "        # Write to the video file, if any.\n",
      "        self._write_video_frame()\n",
      "        # Release the frame.\n",
      "        self._frame = None\n",
      "        self._enteredFrame = False\n",
      "\n",
      "    def write_image(self, filename):\n",
      "        \"\"\"Write the next exited frame to an image file.\"\"\"\n",
      "        self._image_filename = filename\n",
      "\n",
      "    def start_writing_video(self, filename,\n",
      "                            encoding=cv2.cv.CV_FOURCC('I', '4', '2', '0')):\n",
      "        \"\"\"Start writing exited frames to a video file.\"\"\"\n",
      "        self._video_filename = filename\n",
      "        self._video_encoding = encoding\n",
      "\n",
      "    def stop_writing_video(self):\n",
      "        \"\"\"Stop writing exited frames to a video file.\"\"\"\n",
      "        self._video_filename = None\n",
      "        self._video_encoding = None\n",
      "        self._video_writer = None\n",
      "\n",
      "    def _write_video_frame(self):\n",
      "        if not self.is_writing_video:\n",
      "            return\n",
      "        if self._video_writer is None:\n",
      "            fps = self._capture.get(cv2.cv.CV_CAP_PROP_FPS)\n",
      "            if fps <= 0.0:\n",
      "                # The capture's FPS is unknown so use an estimate.\n",
      "                if self._frames_elapsed < 20:\n",
      "                    # Wait until more frames elapse so that the\n",
      "                    # estimate is more stable.\n",
      "                    return\n",
      "                else:\n",
      "                    fps = self._fps_estimate\n",
      "            size = (int(self._capture.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)),\n",
      "                    int(self._capture.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)))\n",
      "            self._video_writer = cv2.VideoWriter(\n",
      "                self._video_filename, self._video_encoding, fps, size)\n",
      "        self._video_writer.write(self._frame)\n",
      "\n",
      "\n",
      "class WindowManager(object):\n",
      "\n",
      "    def __init__(self, window_name, keypress_callback=None):\n",
      "        self.keypress_callback = keypress_callback\n",
      "        self._window_name = window_name\n",
      "        self._is_window_created = False\n",
      "\n",
      "    @property\n",
      "    def is_window_created(self):\n",
      "        return self._is_window_created\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def create_window(self):\n",
      "        self._is_window_created = True\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def show(self, frame):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def destroy_window(self):\n",
      "        self._is_window_created = False\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def process_events(self, key_code):\n",
      "        if self.keypress_callback is not None and key_code != -1:\n",
      "            # Discard any non-ASCII info encoded by GTK.\n",
      "            key_code &= 0xFF\n",
      "            self.keypress_callback(key_code)\n",
      "\n",
      "\n",
      "class OpenCVWindowManager(WindowManager):\n",
      "\n",
      "    def create_window(self):\n",
      "        cv2.namedWindow(self._window_name)\n",
      "        WindowManager.create_window(self)\n",
      "\n",
      "    def show(self, frame):\n",
      "        cv2.imshow(self._window_name, frame)\n",
      "\n",
      "    def destroy_window(self):\n",
      "        cv2.destroyWindow(self._window_name)\n",
      "        WindowManager.destroy_window(self)\n",
      "\n",
      "    def process_events(self):\n",
      "        key_code = cv2.waitKey(1)\n",
      "        WindowManager.process_events(self, key_code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def is_gray(image):\n",
      "    r\"\"\"\n",
      "    Returns True if the image has one channel per pixel.\n",
      "    \"\"\"\n",
      "    return image.ndim < 3\n",
      "\n",
      "\n",
      "def width_height_divided_by(image, divisor):\n",
      "    r\"\"\"\n",
      "    Returns the height and width of the image divided by a value.\n",
      "    \"\"\"\n",
      "    h, w = image.shape[:2]\n",
      "    return h/divisor, w/divisor\n",
      "\n",
      "\n",
      "def draw_bounding_box(image, bounding_box, color=(0, 0, 255), thickness=2):\n",
      "    r\"\"\"\n",
      "    Draws a bounding box on the image using opencv's rectangle function.\n",
      "    \"\"\"\n",
      "    x, y, w, h = bounding_box\n",
      "    cv2.rectangle(image, (x, y), (x+w, y+h),\n",
      "                  color=color, thickness=thickness)\n",
      "\n",
      "\n",
      "def draw_landmarks(image, landmarks):\n",
      "    r\"\"\"\n",
      "    Draws landmarks on the image using opencv's circle function\n",
      "    \"\"\"\n",
      "    for l in landmarks:\n",
      "        cv2.circle(image, (int(l[1]), int(l[0])),\n",
      "                   radius=1, color=(0, 255, 0), thickness=2)\n",
      "\n",
      "\n",
      "def crop_image(image, bounding_box, safe_margin=0.5):\n",
      "    x, y, w, h = bounding_box\n",
      "    bb_range = np.array([x+w, y+h]) - np.array([x, y])\n",
      "    bb_margin = bb_range * safe_margin\n",
      "    extended_bb = bounding_box + np.hstack([-bb_margin, bb_margin])\n",
      "\n",
      "    e_x, e_y, e_w, e_h = np.require(extended_bb, dtype=int)\n",
      "    extended_xy = np.array([e_x, e_y])\n",
      "\n",
      "    cropped_image = image[e_y:e_y+e_h, e_x:e_x+e_w]\n",
      "\n",
      "    xy_difference = np.array([x, y]) - extended_xy\n",
      "    cropped_bb = (xy_difference[0], xy_difference[1], xy_difference[0]+w,\n",
      "                  xy_difference[1]+h)\n",
      "\n",
      "    return cropped_image, cropped_bb, extended_xy\n",
      "\n",
      "\n",
      "def build_bb_from_landmarks(landmarks):\n",
      "    y, x, = minimum = np.min(landmarks, axis=0)\n",
      "    h, w = np.max(landmarks, axis=0) - minimum\n",
      "    return x, y, w, h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import abc\n",
      "import cv2\n",
      "\n",
      "\n",
      "class ObjectDetector(object):\n",
      "    r\"\"\"\n",
      "    Object detector interface\n",
      "    \"\"\"\n",
      "    __metaclass__ = abc.ABCMeta\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def detect(self, image):\n",
      "        r\"\"\"\n",
      "        Detects objects in an image.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "# opencv face classifiers paths\n",
      "haar_default = '/usr/local/Cellar/opencv/2.4.8.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
      "haar_alt = '/cashaarcascades/haarcascade_frontalface_alt.xml'\n",
      "haar_alt2 = '/cashaarcascades/haarcascade_frontalface_alt2.xml'\n",
      "haar_alt_tree = '/cashaarcascades/haarcascade_frontalface_alt_tree.xml'\n",
      "lbp_default = '/lbpcascades/lbpcascade_frontalface'\n",
      "\n",
      "\n",
      "class OpenCVFaceDetector(ObjectDetector):\n",
      "\n",
      "    def __init__(self, classifier=haar_default, scale_factor=1.1,\n",
      "                 min_neighbors=5, flags=cv2.cv.CV_HAAR_SCALE_IMAGE):\n",
      "        # face detector options\n",
      "        self.scale_factor = scale_factor\n",
      "        self.min_neighbors = min_neighbors\n",
      "        self.flags = flags\n",
      "        # face classifier\n",
      "        self._face_classifier = cv2.CascadeClassifier(classifier)\n",
      "\n",
      "    def detect(self, image):\n",
      "        # set minimum face size\n",
      "        min_size = width_height_divided_by(image, 8)\n",
      "        # detect faces and return their bounding boxes\n",
      "        return self._face_classifier.detectMultiScale(\n",
      "            image, self.scale_factor, self.min_neighbors, self.flags,\n",
      "            min_size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "detector = OpenCVFaceDetector()\n",
      "window_manager = OpenCVWindowManager('Menpo-Tracker')\n",
      "capture_manager = CaptureManager(\n",
      "            cv2.VideoCapture('/Users/joan/PhD/DataBases/videos/GoodWillHunting_cut1.mp4'), window_manager, True)\n",
      "\n",
      "eyeClassifier = cv2.CascadeClassifier(\n",
      "               '/usr/local/Cellar/opencv/2.4.8.2/share/OpenCV/haarcascades/haarcascade_eye.xml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import matplotlib as plt\n",
      "\n",
      "success, frame = vidcap.read()\n",
      "frame = frame[:, :, ::-1]\n",
      "frame = cv2.resize(frame, (np.require(frame.shape[1] / 2, dtype=int), np.require(frame.shape[0] / 2, dtype=int)))\n",
      "plt.imshow(frame)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'NoneType' object has no attribute '__getitem__'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-3041042c2f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vidcap.retrieve()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "(False, None)"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create window\n",
      "window_manager.create_window()\n",
      "\n",
      "scale = 3\n",
      "\n",
      "fitting_results_sdm_helen = []\n",
      "for j in range(200):\n",
      "    \n",
      "    # read frame\n",
      "    capture_manager.enter_frame()\n",
      "    frame = capture_manager.frame\n",
      "    # convert to grayscale\n",
      "    image = cv2.cvtColor(frame, cv2.cv.CV_BGR2GRAY)\n",
      "    # resize\n",
      "    image = cv2.resize(image, (np.require(frame.shape[1] / scale, dtype=int), np.require(frame.shape[0] / scale, dtype=int)))\n",
      "      \n",
      "    # detect objects\n",
      "    bounding_boxes = detector.detect(image)\n",
      "    \n",
      "    for bb in bounding_boxes:\n",
      "        # draw the bounding box containing the object\n",
      "        draw_bounding_box(frame, bb * scale)\n",
      "        \n",
      "        # seek an eye in the upper-left part of the face.\n",
      "        x, y, w, h = bb\n",
      "        x2, y2, w2, h2 = (x+w/7, y, w*2/7, h/2)\n",
      "        eye_image = image[y2:y2+h2, x2:x2+w2]\n",
      "        minSize = width_height_divided_by(image, 64)\n",
      "        eye_bounding_boxes = eyeClassifier.detectMultiScale(eye_image, 1.1, 5, cv2.cv.CV_HAAR_SCALE_IMAGE, minSize)\n",
      "        \n",
      "        for eye_bb in eye_bounding_boxes:\n",
      "            subX, subY, subW, subH = eye_bb * scale\n",
      "            draw_bounding_box(frame, np.require(np.array([x+subX, y+subY, subW, subH]), dtype=int))\n",
      "            print 'bye'\n",
      "        \n",
      "        print 'hi'\n",
      "    \n",
      "    capture_manager.exit_frame()\n",
      "    window_manager.process_events()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hi\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bye"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hi\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%debug"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dill as pickle\n",
      "fitter = pickle.load(open('/Users/joan/PhD/Models/sdm.menpo', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}